{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import CIFAR10"
      ],
      "metadata": {
        "id": "DdgAuJqRx5TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load and Preprocess data"
      ],
      "metadata": {
        "id": "TSfoSiF-C98w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = CIFAR10(root = './data', train = True, download = True)\n",
        "test = CIFAR10(root = './data', train = False, download = True)\n"
      ],
      "metadata": {
        "id": "XNPfh_osyOj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "for i in range(len(train)):\n",
        "    x_train.append(np.asarray(train[i][0]))\n",
        "    y_train.append(train[i][1])\n",
        "    i += 1\n",
        "\n",
        "for i in range(len(test)):\n",
        "    x_test.append(np.asarray(testnset[i][0]))\n",
        "    y_test.append(test[i][1])\n",
        "    i += 1\n",
        "\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "x_test = np.asarray(x_test)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "\n",
        "x_nonorm_train = x_train # Save unnormalized data\n",
        "\n",
        "#Normalize Data\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Number of features:\n",
        "feat = 1\n",
        "for d in x_train[0].shape:\n",
        "    feat *= d\n",
        "\n",
        "# Flattening the data:\n",
        "x_train = x_train.reshape(-1, feat, 1)\n",
        "x_nonorm_train = x_nonorm_train.reshape(-1, feat, 1)\n",
        "\n",
        "x_train = np.squeeze(x_train)\n",
        "x_nonorm_train = np.squeeze(x_nonorm_train)\n",
        "y_train = np.squeeze(y_train)\n",
        "y_test = np.squeeze(y_test)"
      ],
      "metadata": {
        "id": "gxOFcbTiyZ-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_pbMwASyhxW",
        "outputId": "d5418872-fc72-4c80-cec7-26cdca211f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3072)\n",
            "(50000,)\n",
            "(10000, 3072)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "2xmPR0hXyoMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, layer_type='D', I = 256, O = 256, gamma = 0):\n",
        "        self.layer_type = layer_type\n",
        "        self.I = I  # input dimensions\n",
        "        self.O = O # output dimensions\n",
        "        self.gamma = gamma\n",
        "\n",
        "        if layer_type == 'D':\n",
        "          # Initialize weights and biases\n",
        "          self.W = np.random.normal(loc = 0.0, scale = np.sqrt(2 / (I + O)), size = (I ,O))\n",
        "          # Biases:\n",
        "          self.b = np.zeros(O)\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "      if self.layer_type == 'R':\n",
        "        return np.maximum(x, 0)   # Relu activation\n",
        "      elif self.layer_type == 'D':\n",
        "        return np.dot(x, self.W) + self.b   # Dense layer\n",
        "      elif self.layer_type == 'LR':  # Leaky relu activation\n",
        "        return np.maximum(x, 0) + self.gamma * np.minimum(x, 0)\n",
        "      elif self.layer_type == 'T':   # Tanh activation\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backpropagation(self, x, grad_y, learning_rate, l2_reg, l1_reg):\n",
        "      if self.layer_type == 'R':\n",
        "        return grad_y * (x > 0)  # Relu derivative * gradient of previous layer\n",
        "\n",
        "      elif self.layer_type == 'D':\n",
        "        grad_x = np.dot(grad_y, self.W.T)\n",
        "        \n",
        "        grad_W = np.dot(x.T, grad_y)\n",
        "        grad_b = grad_y.mean(axis = 0) * x.shape[0]\n",
        "                \n",
        "        # Update weights\n",
        "        self.W = self.W - learning_rate * grad_W - l2_reg * self.W - l1_reg * np.sign(self.W) \n",
        "        self.b = self.b - learning_rate * grad_b \n",
        "        \n",
        "        return grad_x\n",
        "\n",
        "      elif self.layer_type == 'LR':  # Leaky relu derivative * gradient of previous layer\n",
        "        return grad_y * ((x > 0) + self.gamma * (x < 0))\n",
        "      elif self.layer_type == 'T':   # Tanh derivative * gradient of previous layer\n",
        "        return grad_y * (1 - np.square(np.tanh(x)))\n",
        "\n",
        "         "
      ],
      "metadata": {
        "id": "wEBmJIyUynCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "\n",
        "    def __init__( self, layers, epochs = 50, batch_size = 40, epsilon = 1e-6, lr = 0.01, l2_reg=0, l1_reg=0):\n",
        "\n",
        "        self.layers = layers\n",
        "        self.epochs = epochs\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.l2_reg = l2_reg\n",
        "        self.l1_reg = l1_reg\n",
        "        self.losses = []\n",
        "\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.exp(z).sum(axis = -1, keepdims = True)\n",
        "\n",
        "    # Softmax crossentropy loss\n",
        "    def loss(self, z, y ):\n",
        "        return - z[np.arange(len(z)), y] + np.log(np.sum(np.exp(z), axis = -1))\n",
        "\n",
        "    # Get dy\n",
        "    def grad_loss(self, z, y):\n",
        "      \n",
        "        yh = self.softmax(z)\n",
        "\n",
        "        ones = np.zeros_like(z)\n",
        "        ones[np.arange(len(z)), y] = 1\n",
        "        \n",
        "        return (- ones + yh) / z.shape[0]\n",
        "\n",
        "    def feedforward(self, layers,  x):\n",
        "      \n",
        "        activated_outputs = [] # contains [z1, z2, yh]\n",
        "        \n",
        "        for layer in layers:\n",
        "            activated_outputs.append(layer.feedforward(x))\n",
        "            x = activated_outputs[-1]\n",
        "        \n",
        "        return activated_outputs\n",
        "\n",
        "    def get_minibatches(self, X, y, batch_size):\n",
        "        batches_x = []\n",
        "        batches_y = []\n",
        "        num_batches = 0\n",
        "        indices = np.random.permutation(len(X))\n",
        "        for i in range(0, len(X) - batch_size + 1, batch_size):\n",
        "            j = indices[i:i + batch_size]\n",
        "            batches_x.append(X[j])\n",
        "            batches_y.append(y[j])\n",
        "            num_batches += 1\n",
        "\n",
        "        return batches_x, batches_y, num_batches\n",
        "\n",
        "    def fit(self, x, y): #Creates minibatches and fits model\n",
        "      batches_x, batches_y, num_batches = self.get_minibatches(x, y, self.batch_size)\n",
        "      t = 0\n",
        "      while t < self.epochs:\n",
        "        for i in range(num_batches):\n",
        "          batchx = batches_x[i].reshape(-1, 3072)\n",
        "          batchy = batches_y[i].reshape(batchx.shape[0], )\n",
        "          loss = self.train(batchx, batchy)\n",
        "          if i == 0:\n",
        "            self.losses.append(loss)\n",
        "        t += 1\n",
        "\n",
        "    # Performs forward and backward pass on given set of data\n",
        "    def train(self, X, y): \n",
        "\n",
        "        yh = self.feedforward(self.layers, X)\n",
        "        layer_inputs = [X] + yh \n",
        "        uc = yh[-1]\n",
        "        y = y.astype(int)\n",
        "        dy = self.grad_loss(uc, y)\n",
        "        \n",
        "        # Backpropagation:\n",
        "        for i in range(len(self.layers))[::-1]:\n",
        "            layer = self.layers[i]\n",
        "            dy = layer.backpropagation(layer_inputs[i], dy, self.lr, self.l2_reg, self.l1_reg)\n",
        "        loss = self.loss(uc, y)    \n",
        "        return np.mean(loss)\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = self.feedforward(self.layers, X)[-1]\n",
        "        return z.argmax(axis = -1)\n",
        "\n",
        "    def evaluate_acc(self, yh, y):\n",
        "      return np.mean(yh == y)"
      ],
      "metadata": {
        "id": "V8ydzbw8gaNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "bjLfJWNQsosX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Comparison - All 3 models"
      ],
      "metadata": {
        "id": "KofroGkickEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the three models\n",
        "\n",
        "# No hidden layers\n",
        "layers1 = [Layer(I = x_train.shape[1], O = 10)]\n",
        "model1 = MLP(layers=layers1)\n",
        "\n",
        "# One hidden layer\n",
        "layers2 = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(O=10)]\n",
        "model2 = MLP(layers=layers2)\n",
        "\n",
        "\n",
        "# Two hidden layers\n",
        "layers3 = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'), Layer(O=10)]\n",
        "model3 = MLP(layers = layers3)"
      ],
      "metadata": {
        "id": "-XaM2eXIcj1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine Accuracies\n",
        "\n",
        "model1.fit(x_train, y_train)\n",
        "yh1_test = model1.predict(x_test)\n",
        "acc1_test = model1.evaluate_acc(yh1_test, y_test)\n",
        "print(\"MLP1: No Hidden Layer Test Accuracy:\"  + str(acc1_test))\n",
        "\n",
        "model2.fit(x_train, y_train)\n",
        "yh2_test = model2.predict(x_test)\n",
        "acc2_test = model2.evaluate_acc(yh2_test, y_test)\n",
        "print(\"MLP2: Single Hidden Layer Test Accuracy:\"  + str(acc2_test))\n",
        "\n",
        "model3.fit(x_train, y_train)\n",
        "yh3_test = model3.predict(x_test)\n",
        "acc3_test = model3.evaluate_acc(yh3_test, y_test)\n",
        "print(\"MLP3: Two Hidden Layers Test Acc:\" + str(acc3_test))"
      ],
      "metadata": {
        "id": "GNZyuA1JfJw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Hidden Layer "
      ],
      "metadata": {
        "id": "g_OFnqOpWh8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide training set into training and validation\n",
        "\n",
        "x_train1, x_valid = x_train[:-10000], x_train[-10000:]\n",
        "y_train1, y_valid = y_train[:-10000], y_train[-10000:]"
      ],
      "metadata": {
        "id": "KptCmrjGqnfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Best Leaky Relu Gamma"
      ],
      "metadata": {
        "id": "ShT-p7YNwOS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine gamma that gives highest validation accuracy\n",
        "\n",
        "\n",
        "gammas = [0.0001, 0.001, 0.01, 0.1]\n",
        "accuracies = []\n",
        "\n",
        "for i in range(len(gammas)):\n",
        "  layers_leaky = [Layer(I=x_train.shape[1]), Layer(layer_type='LR', gamma=gammas[i]), Layer(), Layer(layer_type='LR', gamma=gammas[i]),Layer(O=10)]\n",
        "  model = MLP(layers=layers_leaky)\n",
        "  model.fit(x_train1, y_train1)\n",
        "  yh = model.predict(x_valid)\n",
        "  acc = model.evaluate_acc(yh, y_valid)\n",
        "  accuracies.append(acc)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(gammas, accuracies, marker= '.', alpha=.998)\n",
        "plt.xlabel('Gamma')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy as a function of gamma - Two hidden layers, Leaky Relu')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T60Ho6LrwOAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch size"
      ],
      "metadata": {
        "id": "dsvT4TwOpkip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine batch size that gives highest validation accuracy\n",
        "\n",
        "\n",
        "batch_sizes = [40, 100, 300, 700, 1000]\n",
        "accs_r = []\n",
        "accs_lr = []\n",
        "accs_t = []\n",
        "\n",
        "\n",
        "for i in range(len(batch_sizes)):\n",
        "  layers_relu = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'),Layer(O=10)]\n",
        "  layers_tanh = [Layer(I=x_train.shape[1]), Layer(layer_type='T'), Layer(), Layer(layer_type='T'),Layer(O=10)]\n",
        "  layers_leaky = [Layer(I=x_train.shape[1]), Layer(layer_type='LR', gamma=0.001), Layer(), Layer(layer_type='LR', gamma=0.001),Layer(O=10)]\n",
        "  modelr = MLP(layers_relu, batch_size=batch_sizes[i], epochs=10)\n",
        "  modellr = MLP(layers_leaky, batch_size=batch_sizes[i], epochs=10)\n",
        "  modelt = MLP(layers_tanh, batch_size = batch_sizes[i],epochs=10)\n",
        "  modelr.fit(x_train1, y_train1)\n",
        "  modellr.fit(x_train1, y_train1)\n",
        "  modelt.fit(x_train1, y_train1)\n",
        "  yhr = modelr.predict(x_valid)\n",
        "  yhlr = modellr.predict(x_valid)\n",
        "  yht = modelt.predict(x_valid)\n",
        "  accs_r.append(modelr.evaluate_acc(yhr, y_valid))\n",
        "  accs_lr.append(modellr.evaluate_acc(yhlr, y_valid))\n",
        "  accs_t.append(modelt.evaluate_acc(yht, y_valid))\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(batch_sizes, accs_r, marker= '.', color=\"blue\", alpha=.998, label=\"Relu\")\n",
        "plt.plot(batch_sizes, accs_lr, marker= '.', color=\"red\", alpha=.998, label=\"Leaky Relu\")\n",
        "plt.plot(batch_sizes, accs_t, marker= '.', color=\"green\", alpha=.998, label = \"Tanh\")\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy as a function of batch size')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZD-fsaeMp61t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate"
      ],
      "metadata": {
        "id": "Q_dFPW2VuhPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine batch size that gives highest validation accuracy\n",
        "\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "accs_r = []\n",
        "accs_lr = []\n",
        "accs_t = []\n",
        "\n",
        "\n",
        "for i in range(len(learning_rates)):\n",
        "  layers_relu = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'),Layer(O=10)]\n",
        "  layers_tanh = [Layer(I=x_train.shape[1]), Layer(layer_type='T'), Layer(), Layer(layer_type='T'),Layer(O=10)]\n",
        "  layers_leaky = [Layer(I=x_train.shape[1]), Layer(layer_type='LR', gamma=0.001), Layer(), Layer(layer_type='LR', gamma=0.001),Layer(O=10)]\n",
        "  modelr = MLP(layers_relu, lr = learning_rates[i])\n",
        "  modellr = MLP(layers_leaky, lr = learning_rates[i])\n",
        "  modelt = MLP(layers_tanh, lr = learning_rates[i])\n",
        "  modelr.fit(x_train1, y_train1)\n",
        "  modellr.fit(x_train1, y_train1)\n",
        "  modelt.fit(x_train1, y_train1)\n",
        "  yhr = modelr.predict(x_valid)\n",
        "  yhlr = modellr.predict(x_valid)\n",
        "  yht = modelt.predict(x_valid)\n",
        "  accs_r.append(modelr.evaluate_acc(yhr, y_valid))\n",
        "  accs_lr.append(modellr.evaluate_acc(yhlr, y_valid))\n",
        "  accs_t.append(modelt.evaluate_acc(yht, y_valid))\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(learning_rates, accs_r, marker= '.', color=\"blue\", alpha=.998, label=\"Relu\")\n",
        "plt.plot(learning_rates, accs_lr, marker= '.', color=\"red\", alpha=.998, label=\"Leaky Relu\")\n",
        "plt.plot(learning_rates, accs_t, marker= '.', color=\"green\", alpha=.998, label = \"Tanh\")\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy as a function of learning rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hw6exSn2p6iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Accuracy of Tanh and Leaky Relu"
      ],
      "metadata": {
        "id": "iRHBAQVGshC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Models\n",
        "\n",
        "layers_leaky = [Layer(I=x_train.shape[1]), Layer(layer_type='LR', gamma=0.0001), Layer(), Layer(layer_type='LR', gamma=0.0001),Layer(O=10)]\n",
        "model_lr = MLP(layers=layers_leaky)\n",
        "layers_tanh = [Layer(I=x_train.shape[1]), Layer(layer_type='T'), Layer(), Layer(layer_type='T'),Layer(O=10)]\n",
        "model_t = MLP(layers=layers_tanh)\n"
      ],
      "metadata": {
        "id": "pZ7eNvSPaWw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the 50 000 sample training set, and predict the test outputs\n",
        "\n",
        "model_lr.fit(x_train, y_train)\n",
        "print(\"Leaky relu model fit done\")\n",
        "model_t.fit(x_train, y_train)\n",
        "print(\"Tanh model fit done\")\n",
        "\n",
        "yh_lr = model_lr.predict(x_test)\n",
        "yh_t = model_t.predict(x_test)\n",
        "\n",
        "print(\"Test accuracy for leaky relu:\" + str(model_lr.evaluate_acc(yh_lr, y_test)) + \"\\nTest accuracy for tanh :\" + str(model_t.evaluate_acc(yh_t, y_test)))\n"
      ],
      "metadata": {
        "id": "I3Q2SRDnGT3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###L2 Regularization"
      ],
      "metadata": {
        "id": "pPhvyboBb04m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l2_regs = [0.000001, 0.00001, 0.0001]\n",
        "accuracies = []\n",
        "\n",
        "for i in range(len(l2_regs)):\n",
        "  layers_relu = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'),Layer(O=10)]\n",
        "  model = MLP(layers=layers_relu, l2_reg = l2_regs[i])\n",
        "  model.fit(x_train1, y_train1)\n",
        "  yh = model.predict(x_valid)\n",
        "  acc = model.evaluate_acc(yh, y_valid)\n",
        "  accuracies.append(acc)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(l2_regs, accuracies, marker= '.', alpha=.998)\n",
        "plt.xlabel('L2 Regularization Strength')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy as a function of L2 Regularization Strength')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p-BOdTE_b0nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L1 Regularization"
      ],
      "metadata": {
        "id": "lHh3VHlOcYrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_regs = [0.001, 0.01, 0.1]\n",
        "accuracies = []\n",
        "\n",
        "for i in range(len(l2_regs)):\n",
        "  layers_relu = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'),Layer(O=10)]\n",
        "  model = MLP(layers=layers_relu)\n",
        "  model.fit(x_train1, y_train1)\n",
        "  yh = model.predict(x_valid)\n",
        "  acc = model.evaluate_acc(yh, y_valid)\n",
        "  accuracies.append(acc)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(l2_regs, accuracies, marker= '.', alpha=.998)\n",
        "plt.xlabel('L2 Regularization Strength')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Validation Accuracy as a function of L2 Regularization Strength')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HrjcshDFca5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unnormalized Data"
      ],
      "metadata": {
        "id": "w3nn4YFtpsWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers_relu1 = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'),Layer(O=10)]\n",
        "modelrelu1 = MLP(layers=layers_relu)\n",
        "\n",
        "modelrelu1.fit(x_nonorm_train, y_train)\n",
        "\n",
        "yh6 = modelrelu1.predict(x_nonorm_test)\n",
        "print(\"Test Accuracy of unnormalized data:\" + str(modelrelu1.evaluate_acc(yh6, y_test)))"
      ],
      "metadata": {
        "id": "7saep17xqBsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Curves: Relu"
      ],
      "metadata": {
        "id": "2ZXgxy5VqEqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers_relu = [Layer(I=x_train.shape[1]), Layer(layer_type='R'), Layer(), Layer(layer_type='R'),Layer(O=10)]\n",
        "model = MLP(layers=layers_relu, epochs=50)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "losses = model.losses\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(losses, marker= '.', alpha=.998)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Training curve - Two hidden layers, Relu')\n",
        "plt.show()\n",
        "\n",
        "yh = model.predict(x_train)\n",
        "acc = model.evaluate_acc(yh, y_train)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "r-PvWmB_qEgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}