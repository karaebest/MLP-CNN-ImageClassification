{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgW2yin9GKaD"
      },
      "outputs": [],
      "source": [
        "#Given an image, can we predict the correct class of this image?\n",
        "\n",
        "# The images are very small (32x32) and by visualizing them you will notice how difficult it is to distinguish them even for a human.\n",
        "\n",
        "# In this notebook we are going to build a CNN model that can classify images of various objects. We have 10 class of images:\n",
        "\n",
        "# Airplane\n",
        "# Automobile\n",
        "# Bird\n",
        "# Cat\n",
        "# Deer\n",
        "# Dog\n",
        "# Frog\n",
        "# Horse\n",
        "# Ship\n",
        "# Truck\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "#Load the data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualization"
      ],
      "metadata": {
        "id": "iHpRISDJGVXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the labels of the dataset\n",
        "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "          'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Let's view more images in a grid format\n",
        "# Define the dimensions of the plot grid \n",
        "W_grid = 10\n",
        "L_grid = 10\n",
        "\n",
        "# fig, axes = plt.subplots(L_grid, W_grid)\n",
        "# subplot return the figure object and axes object\n",
        "# we can use the axes object to plot specific figures at various locations\n",
        "\n",
        "fig, axes = plt.subplots(L_grid, W_grid, figsize = (17,17))\n",
        "\n",
        "axes = axes.ravel() # flaten the 15 x 15 matrix into 225 array\n",
        "\n",
        "n_train = len(X_train) # get the length of the train dataset\n",
        "\n",
        "# Select a random number from 0 to n_train\n",
        "for i in np.arange(0, W_grid * L_grid): # create evenly spaces variables \n",
        "\n",
        "    # Select a random number\n",
        "    index = np.random.randint(0, n_train)\n",
        "    # read and display an image with the selected index    \n",
        "    axes[i].imshow(X_train[index,1:])\n",
        "    label_index = int(y_train[index])\n",
        "    axes[i].set_title(labels[label_index], fontsize = 8)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.subplots_adjust(hspace=0.4)"
      ],
      "metadata": {
        "id": "JFRRWDMxGXfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "no2QD-ZOGpDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Transform target variable into one-hotencoding\n",
        "y_cat_train = to_categorical(y_train, 10)\n",
        "y_cat_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "tXyhVzaMGq-j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_cat_train"
      ],
      "metadata": {
        "id": "_fq53YwjGtkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Model"
      ],
      "metadata": {
        "id": "Cr29NszIGuox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the shape of the input data\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "\n",
        "# Set the size of the convolutional kernel\n",
        "KERNEL_SIZE = (3, 3)\n",
        "\n",
        "# Initialize a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a convolutional layer with 32 filters, ReLU activation, and same padding\n",
        "model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "\n",
        "# Add batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add another convolutional layer with 32 filters, ReLU activation, and same padding\n",
        "model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "\n",
        "# Add another batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add a max pooling layer with a pool size of (2,2)\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a dropout layer with a rate of 0.25\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add a convolutional layer with 64 filters, ReLU activation, and same padding\n",
        "model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "\n",
        "# Add batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add another convolutional layer with 64 filters, ReLU activation, and same padding\n",
        "model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "\n",
        "# Add another batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add a max pooling layer with a pool size of (2,2)\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a dropout layer with a rate of 0.25\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add a convolutional layer with 128 filters, ReLU activation, and same padding\n",
        "model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "\n",
        "# Add batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add another convolutional layer with 128 filters, ReLU activation, and same padding\n",
        "model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "\n",
        "# Add another batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add a max pooling layer with a pool size of (2,2)\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a dropout layer with a rate of 0.25\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten the output of the previous layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add a dropout layer with a rate of 0.25\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Add a dense layer with 10 neurons and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Define the metrics to be used during model training and evaluation\n",
        "METRICS = [\n",
        "    'accuracy',\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "# Compile the model with categorical crossentropy loss, Adam optimizer, and the defined metrics\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)\n"
      ],
      "metadata": {
        "id": "ftVCR3L6GvpW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "X71p3raeG2FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping"
      ],
      "metadata": {
        "id": "vgXdZhWdG5e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)"
      ],
      "metadata": {
        "id": "vDIm5ZBDG65t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentations"
      ],
      "metadata": {
        "id": "XUKRiJuFG_QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size for data augmentation\n",
        "batch_size = 32\n",
        "\n",
        "# Define an ImageDataGenerator object for data augmentation\n",
        "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\n",
        "# Generate batches of augmented data using the ImageDataGenerator object\n",
        "train_generator = data_generator.flow(X_train, y_cat_train, batch_size)\n",
        "\n",
        "# Calculate the number of steps (batches) per epoch\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "# Train the model using the augmented data generator\n",
        "# Fit the model to the data generator for 50 epochs and track the validation accuracy\n",
        "# Set the steps_per_epoch argument to the calculated number of steps\n",
        "# Set the validation_data argument to the test data\n",
        "r = model.fit(train_generator, \n",
        "              epochs=5,\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              validation_data=(X_test, y_cat_test))\n",
        "             \n"
      ],
      "metadata": {
        "id": "6aRu3Z52H1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 16))\n",
        "\n",
        "plt.subplot(4, 2, 1)\n",
        "plt.plot(r.history['loss'], label='Loss')\n",
        "plt.plot(r.history['val_loss'], label='val_Loss')\n",
        "plt.title('Loss Function Evolution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "plt.plot(r.history['accuracy'], label='accuracy')\n",
        "plt.plot(r.history['val_accuracy'], label='val_accuracy')\n",
        "plt.title('Accuracy Function Evolution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "plt.plot(r.history['precision'], label='precision')\n",
        "plt.plot(r.history['val_precision'], label='val_precision')\n",
        "plt.title('Precision Function Evolution')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "plt.plot(r.history['recall'], label='recall')\n",
        "plt.plot(r.history['val_recall'], label='val_recall')\n",
        "plt.title('Recall Function Evolution')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Idq5DVESHEXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 5 - Note 2"
      ],
      "metadata": {
        "id": "zPQSliWmOz3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a learning rate scheduler\n",
        "initial_learning_rate = 0.01\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))\n",
        "\n",
        "# Plot the learning rate vs. test accuracy\n",
        "learning_rates = initial_learning_rate * np.power(0.96, np.arange(10))\n",
        "test_accs = history.history['val_accuracy']\n",
        "plt.plot(learning_rates, test_accs)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Test accuracy')\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set and print the accuracy\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "id": "Od0idlDTOu7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}